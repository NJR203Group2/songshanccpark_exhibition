{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T3MhTai2S-9M"
      },
      "outputs": [],
      "source": [
        "# é¿å…æ²’æœ‰ç›¸é—œæ¨¡çµ„\n",
        "!pip install easyocr google-genai google-api-python-client opencv-python-headless"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B08jfInsNHCh"
      },
      "source": [
        "# Use Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHB4WU7smTIC"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# I. æ ¸å¿ƒ AI æœå‹™ (Gemini/RAG) & OCR\n",
        "# ====================================================================\n",
        "import cv2                        # è™•ç†åœ–ç‰‡ (OCR)\n",
        "import json                       # è™•ç† JSON æ ¼å¼\n",
        "from google import genai          # Gemini API\n",
        "from google.genai import types    # Gemini çµæ§‹åŒ–è¼¸å‡º Schema\n",
        "from googleapiclient.discovery import build # Google æœå°‹ API\n",
        "from google.colab import drive # æˆæ¬Šå¸³è™Ÿ\n",
        "import easyocr                    # OCR åœ–ç‰‡æ–‡å­—è­˜åˆ¥\n",
        "from google.genai.errors import APIError # è™•ç† Gemini API éŒ¯èª¤\n",
        "\n",
        "# ====================================================================\n",
        "# II. ç¶²è·¯è«‹æ±‚ (Web/HTTP) èˆ‡è§£æ\n",
        "# ====================================================================\n",
        "from bs4 import BeautifulSoup as bs # HTML è§£æ\n",
        "import requests as req              # HTTP è«‹æ±‚\n",
        "from urllib.parse import urljoin, urlparse    # URL çµ„åˆ\n",
        "\n",
        "# ====================================================================\n",
        "# III. æª”æ¡ˆ/ç³»çµ±èˆ‡ç’°å¢ƒè®Šæ•¸ç®¡ç†\n",
        "# ====================================================================\n",
        "import os                         # è®€å–ç’°å¢ƒè®Šæ•¸\n",
        "import re                         # æ­£è¦è¡¨é”å¼\n",
        "from pathlib import Path as pp    # æª”æ¡ˆè·¯å¾‘æ“ä½œ\n",
        "import time                       # åŸ·è¡Œå»¶é²\n",
        "from google.colab import userdata  # è¼‰å…¥ Secrets KEY Valueï¼šcolabç’°å¢ƒä½¿ç”¨\n",
        "import copy # æª”æ¡ˆè™•ç†\n",
        "# ====================================================================\n",
        "# IV. è³‡æ–™ã€æ™‚é–“èˆ‡éš¨æ©Ÿæ€§\n",
        "# ====================================================================\n",
        "import random as rd               # éš¨æ©Ÿå»¶é²æ™‚é–“\n",
        "from datetime import datetime     # è™•ç†æ—¥æœŸèˆ‡æ™‚é–“"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvw3ujd-NYRI"
      },
      "source": [
        "# Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdsMCSJpNswD"
      },
      "source": [
        "## å±•è¦½åŸºç¤è³‡è¨Šçˆ¬èŸ² - ä¸»è¦è³‡è¨Šä¾†æº"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPBVO_AGNjI-"
      },
      "outputs": [],
      "source": [
        "def baseinfo(pageurl, headers) -> list[dict]:\n",
        "    baseans = [] # çˆ¬å‡ºä¾†è³‡è¨Šå„²å­˜ç”¨\n",
        "    # img_path =[] # åœ–ç‰‡è·¯å¾‘\n",
        "    for ccnt, i in enumerate(pageurl):\n",
        "        infolist = {\n",
        "        'strt_dt': None, # é–‹å§‹æ—¥æœŸ\n",
        "        'end_dt': None, # çµæŸæ—¥æœŸ\n",
        "        'name': '', # å±•è¦½åç¨±\n",
        "        'article': '', # é‡é»æ•˜è¿°\n",
        "        'big_imgurl': '',   # é€™è£¡æ”¾åœ–ç‰‡çš„äºŒé€²åˆ¶å…§å®¹ï¼Œç­‰ç­‰æ¯å€‹éƒ½æ”¾é€²å»è¾¨è­˜å…§å®¹ï¼Œå–å‡ºæœ‰ç”¨è³‡è¨Š\n",
        "        'loc': '', # åœ°é»\n",
        "        'pagetext': '', # å±•è¦½å…§å®¹æ•˜è¿°\n",
        "        'pageurl': i, # å±•è¦½å…§å®¹ç¶²å€\n",
        "        'pageimgurl': None\n",
        "        }\n",
        "\n",
        "        resi = req.get(i, headers)\n",
        "        soup = bs(resi.text, 'html.parser')\n",
        "\n",
        "        infolist['name'] = soup.select_one('div.news_inner p.inner_title').get_text(strip = True)\n",
        "        infolist['strt_dt']= soup.select_one('div.under > p.montsrt').get_text(strip = True).split(' - ')[0]\n",
        "        infolist['end_dt']= soup.select_one('div.under > p.montsrt').get_text(strip = True).split(' - ')[1]\n",
        "        infolist['article']= soup.select_one('article.big_article').get_text(strip = True)\n",
        "        infolist['big_imgurl']= urljoin(jpgparpath, soup.select_one('img.big_img').get('src'))\n",
        "        infolist['loc']= soup.select_one('p.place').get_text(strip = True)\n",
        "        infolist['pagetext']= soup.select_one('section').get_text('\\n', strip = True)\n",
        "        infolist['pageimgurl'] = [imgtag.get('src') for imgtag in soup.select('section img')]\n",
        "\n",
        "        # col.update_one({'pageurl': infolist['pageurl']}, {'$set': infolist}, upsert = True) # å­˜é€²DB\n",
        "        # col.create_index('pageurl', unique = True) # è¨­å®šindex\n",
        "        # col.create_index([('article', 'text'), ('pagetext', 'text')]) # è¨­å®šindex > æ–‡å­—ç´¢å¼•\n",
        "        save = (download_img(jpgparpath,\n",
        "                             infolist['big_imgurl'],\n",
        "                             save_dir = str(datetime.strftime(datetime.today(), '%Y_%m_%d')) + '_images', # æ–°å»ºè³‡æ–™åŒ£ï¼Œä»¥ç•¶å¤©æ—¥æœŸåšè³‡æ–™åŒ£åç¨±\n",
        "                             headers = headers,\n",
        "                             img_nm = infolist['name'])) # åœ–ç‰‡ä¸‹è¼‰ï¼Œä¸¦ç”¨æ¨™é¡Œåç¨±ç•¶ä½œæª”å\n",
        "        for idx, imgurl in enumerate(infolist['pageimgurl']):\n",
        "            save_inner = (download_img(jpgparpath,\n",
        "                             imgurl,\n",
        "                             save_dir = str(datetime.strftime(datetime.today(), '%Y_%m_%d')) + '_images/' + f'{infolist['name']}', # æ–°å»ºå­è³‡æ–™åŒ£ï¼ŒåŠ å…¥å±•è¦½åç¨±åšè³‡æ–™åŒ£åç¨±\n",
        "                             headers = headers,\n",
        "                             img_nm = urlparse(imgurl).path.split('/')[-1].split('.')[0]))\n",
        "            infolist['pageimgurl'][idx] = save_inner if save_inner else None # ç½®æ›å¤–éƒ¨ç¶²å€æˆå„²å­˜çš„å…§éƒ¨ç¶²å€\n",
        "        print(f'[{ccnt + 1}/{len(pageurl)}] {infolist['name']}') # å­˜é€² MongoDBï¼Œåœ–ç‰‡å·²ä¸‹è¼‰ {save}\n",
        "        baseans.append(infolist)\n",
        "        time.sleep(rd.randint(1, 15))\n",
        "    return baseans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsNEoFW2Nx1c"
      },
      "source": [
        "## åœ–ç‰‡ä¸‹è¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3OR8UYcN0id"
      },
      "outputs": [],
      "source": [
        "# ImageDownload Function\n",
        "def download_img(base_url: str,\n",
        "                 img_src: str,\n",
        "                 save_dir: str = 'images',\n",
        "                 headers: dict = None,\n",
        "                 img_nm: str = 'unnamed') -> pp | None:\n",
        "    '''\n",
        "        base_url (str): ç¶²é çš„åŸºæº–ç¶²å€ï¼Œç”¨ä¾†è™•ç†ç›¸å°è·¯å¾‘ (ä¾‹: https://example.com/page.html)\n",
        "        img_src (str): <img> æ¨™ç±¤è£¡çš„ src\n",
        "        save_dir (str): å­˜æ”¾åœ–ç‰‡çš„è³‡æ–™å¤¾åç¨±ï¼Œé è¨­ 'images'\n",
        "        headers (dict): requests çš„ HTTP æ¨™é ­\n",
        "    '''\n",
        "\n",
        "    DEFAULT_USER_AGENT = ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
        "    # æª¢æŸ¥user-agentè®Šæ•¸\n",
        "    if headers is None:\n",
        "        current_headers = globals().get('hd', {'user-agent': DEFAULT_USER_AGENT})\n",
        "    else:\n",
        "        current_headers = headers\n",
        "\n",
        "    # æŠŠ base_url å’Œ img_src åˆä½µï¼Œç¢ºä¿æ‹¿åˆ°å®Œæ•´çš„åœ–ç‰‡ç¶²å€\n",
        "    img_url = urljoin(base_url, img_src)\n",
        "\n",
        "    # ç”¨ç¶²é æ¨™é¡Œç•¶ä½œæª”æ¡ˆåç¨±ï¼Œæ’é™¤æ‰windowséæ³•çš„æª”åç¬¦è™Ÿ\n",
        "    illegal_chars = r'[<>:\"/\\\\|?*ï¼\\s]+'\n",
        "    img_name = img_nm.strip().strip('_')\n",
        "    filename = re.sub(illegal_chars, '_', img_name) + '.jpg'\n",
        "\n",
        "    # å»ºç«‹å­˜æ”¾åœ–ç‰‡çš„è³‡æ–™å¤¾ (å¦‚æœä¸å­˜åœ¨å°±å»ºç«‹)\n",
        "    save_path = pp(save_dir)\n",
        "    save_path = filepath / save_path.with_name(re.sub(illegal_chars, '_', save_path.name))\n",
        "    save_path.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "    # ç¢ºèªæª”æ¡ˆè·¯å¾‘ï¼Œä¸¦æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡æª”å\n",
        "    file_path = save_path / filename\n",
        "    counter = 1\n",
        "    while file_path.exists():  # å¦‚æœæª”æ¡ˆå·²ç¶“å­˜åœ¨ï¼Œå°±åœ¨æª”åå¾Œé¢åŠ æ•¸å­—é¿å…è¦†è“‹\n",
        "        stem = file_path.stem      # ä¸åŒ…å«å‰¯æª”åçš„éƒ¨åˆ†\n",
        "        suffix = file_path.suffix  # å‰¯æª”å (ä¾‹å¦‚ .jpg)\n",
        "        file_path = save_path / f'{stem}_{counter}{suffix}'  # ç”¢ç”Ÿæ–°çš„æª”å\n",
        "        counter += 1\n",
        "\n",
        "    try:\n",
        "        resp = req.get(img_url, headers = current_headers, timeout=10)\n",
        "        resp.raise_for_status()  # å¦‚æœç‹€æ…‹ç¢¼ä¸æ˜¯ 200ï¼Œå°±ç›´æ¥ä¸Ÿå‡ºéŒ¯èª¤\n",
        "        # æŠŠä¸‹è¼‰å›ä¾†çš„äºŒé€²ä½å…§å®¹å¯«åˆ°æª”æ¡ˆ\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(resp.content)\n",
        "        return file_path  # æˆåŠŸæ™‚å›å‚³åœ–ç‰‡å­˜æ”¾çš„å®Œæ•´è·¯å¾‘\n",
        "\n",
        "    except Exception as e:\n",
        "        # å¦‚æœä¸‹è¼‰å¤±æ•—ï¼Œå›å‚³è¨Šæ¯\n",
        "        print(f'[{datetime.now()}] {img_url} -> {e}\\n')\n",
        "        return None  # å¤±æ•—å°±å›å‚³ None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9wg5n6bbcQx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgaMcyPUN3l4"
      },
      "source": [
        "## OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx4l0NLpN708"
      },
      "outputs": [],
      "source": [
        "def eocr_process(graph_path: pp, reader: easyocr.Reader) -> str:\n",
        "    '''\n",
        "    ä½¿ç”¨ EasyOCR è®€å–å¤šæ¬„ä½åœ–ç‰‡ï¼Œä¸¦å°‡çµæœæ ¼å¼åŒ–ç‚ºæ˜“æ–¼ AI æå–çš„ç´”æ–‡å­—å­—ä¸²\n",
        "    '''\n",
        "    if not graph_path.exists():\n",
        "        return f'Error åœ–ç‰‡æª”æ¡ˆæœªæ‰¾åˆ°æ–¼è·¯å¾‘: {graph_path}'\n",
        "\n",
        "    try:\n",
        "        # 1. è®€å–åœ–ç‰‡\n",
        "        image = cv2.imread(str(graph_path))\n",
        "        if image is None:\n",
        "            return 'Error éŒ¯èª¤ï¼šç„¡æ³•è®€å–åœ–ç‰‡ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼æˆ–æ˜¯å¦æå£ã€‚'\n",
        "\n",
        "        h, w, _ = image.shape # åœ–ç‰‡shape:é«˜ã€å¯¬ã€é¡è‰²ï¼›ç¬¬ä¸‰è€…ç‚ºBGRï¼Œç¶­åº¦3ï¼Œä½†é€™è£¡ä¸éœ€è¦ä½¿ç”¨\n",
        "\n",
        "        # 2. åŸ·è¡Œ OCR\n",
        "        results = reader.readtext(image, detail = 1)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f'Error EasyOCR æˆ– OpenCV ç™¼ç”ŸéŒ¯èª¤: {e}'\n",
        "\n",
        "    # 2.1 **åˆ†å·¦å³å…©é‚Šå€å¡Š** çš„å‹•æ…‹åƒæ•¸è¨­å®š================================\n",
        "    # è¨­ç½®ç‚ºåœ–ç‰‡é«˜åº¦çš„ 2%ã€‚å¦‚æœæ–‡å­—å¾ˆå°ï¼Œå¯èƒ½éœ€è¦é™ä½åˆ° 0.01ã€‚åˆ¤æ–·å…©å€‹æ–‡å­—å¡Šæ˜¯å¦å±¬æ–¼åŒä¸€è¡Œçš„ Y è»¸è·é›¢\n",
        "    row_tolerance = h * 0.016\n",
        "\n",
        "    # å¦‚æœåˆ†éš”ç·šä¸åœ¨ä¸­é–“ï¼Œä¾‹å¦‚å·¦æ¬„ä½” 60%ï¼Œå‰‡è¨­ç‚º 0.6ã€‚åˆ¤æ–·æ–‡å­—å¡Šå±¬æ–¼å·¦æ¬„é‚„æ˜¯å³æ¬„çš„åˆ†ç•Œç·š\n",
        "    center_x_factor = 0.5 # ä¸­é–“åˆ†éš”ç·šåˆ¤æ–·\n",
        "    center_x = w * center_x_factor\n",
        "\n",
        "    processed_data = [] # ç”¨ä¾†å„²å­˜æ‰€æœ‰å·²ç¶“ç¢ºå®šç‚ºä¸€è¡Œçš„æ•¸æ“šçµæ§‹\n",
        "\n",
        "    # æ ¹æ“š X åº§æ¨™åˆ†è¡Œä¸¦æ¨™è¨˜å·¦å³æ¬„ä½\n",
        "    for bbox, text, conf in results: # åº§æ¨™é»(å…±æœ‰å››é»å·¦ä¸Š[0]ã€å³ä¸Š[1]ã€å³ä¸‹[2]ã€å·¦ä¸‹[3]é †æ™‚é‡)ã€æ–‡å­—å…§å®¹ã€ä¿¡å¿ƒç¨‹åº¦\n",
        "        # å–å¾—é‚Šç•Œæ¡†çš„ä¸­å¿ƒé» Y å’Œ X åº§æ¨™ï¼Œä¸¦ä¸”æŠŠæ¯å€‹bboxå’Œtextéƒ½æ‹¿å‡ºä¾†åˆ¤æ–·\n",
        "        y_center = (bbox[0][1] + bbox[2][1]) / 2 # åˆ¤æ–·è¾¨è­˜å€å¡Šçš„Yè»¸ä¸­å¿ƒé»\n",
        "        x_center = (bbox[0][0] + bbox[1][0]) / 2 # åˆ¤æ–·è¾¨è­˜å€å¡Šçš„Xè»¸ä¸­å¿ƒé»\n",
        "\n",
        "        # æ ¹æ“š X åº§æ¨™åˆ¤æ–·æ¬„ä½\n",
        "        column = 'left' if x_center < center_x else 'right' # å¦‚æœè©²åˆ¤æ–·å€å¡Šï¼Œè½åœ¨ä¸­ç·šå·¦é‚Šï¼Œå‰‡ä»£è¡¨æ˜¯å·¦å´çš„å€å¡Šï¼Œåä¹‹äº¦ç„¶\n",
        "\n",
        "        # åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€è¡Œ\n",
        "        merged = False # åˆä½µé–‹é—œ\n",
        "        for item in processed_data:\n",
        "            if abs(y_center - item['y']) < row_tolerance: # å¦‚æœYè»¸å’Œè©²è¼ªçš„å€å¡ŠYè»¸ä¸­å¿ƒå°æ–¼åˆ¤æ–·è·é›¢ï¼Œå‰‡åˆ¤å®šç‚ºåŒä¸€å¥è©±ï¼Œå¦‚æœæ˜¯åŒä¸€è¡Œï¼Œå°‡ç•¶å‰çš„æ–‡å­—å¡Šæ·»åŠ åˆ°å·²å­˜åœ¨çš„itemä¸­\n",
        "                item['texts'].append({'text': text, 'x': x_center, 'col': column, 'y' : y_center})\n",
        "                merged = True\n",
        "                break # æ‰¾åˆ°æ‡‰è©²æ”¾åˆ°å“ªä¸€è¡Œå¾Œï¼Œå‰‡è·³å‡ºæ­¤è¼ªå¾ªç’°\n",
        "\n",
        "        if not merged: # å¦‚æœè¿´åœˆçµæŸéƒ½æ‰¾ä¸åˆ°åŒ¹é…çš„è¡Œï¼Œå‰‡å‰µå»ºä¸€å€‹æ–°çš„è¡Œ\n",
        "            processed_data.append({\n",
        "                'y': y_center,\n",
        "                'texts': [{'text': text, 'x': x_center, 'col': column, 'y' : y_center}]\n",
        "            }) # ç´€éŒ„å€å¡ŠYè»¸ä¸­å¿ƒé»ã€è¾¨è­˜å‡ºä¾†çš„æ–‡å­—å…§å®¹ã€å€å¡ŠXè»¸ä¸­å¿ƒé»ã€åˆ¤æ–·å¾Œæ˜¯lefté‚„æ˜¯right\n",
        "\n",
        "    # æ’åºèˆ‡æ ¼å¼åŒ–è¼¸å‡º\n",
        "    processed_data = sorted(processed_data, key = lambda r : r['y']) # å–å‡º yå€¼é€²è¡Œæ’åº\n",
        "\n",
        "    left_column_output = ''\n",
        "    right_column_output = ''\n",
        "\n",
        "    for row in processed_data:\n",
        "        # å°åŒä¸€è¡Œåˆ—çš„æ–‡å­—ï¼Œä¾ X åº§æ¨™æ’åºï¼Œåˆ¤å®šå®ƒå±¬æ–¼å“ªä¸€å€‹è¡Œ\n",
        "        left_texts_raw = [t for t in row['texts'] if t['col'] == 'left']\n",
        "        right_texts_raw = [t for t in row['texts'] if t['col'] == 'right']\n",
        "        # å…ˆæŒ‰ X è»¸æ’åºï¼Œè‹¥ X ç›¸åŒï¼Œå‰‡ç”¨ Y è»¸ç¢ºä¿å‚ç›´é †åº\n",
        "        sorted_left_texts = sorted(left_texts_raw, key=lambda t: (t['x'], t['y']))\n",
        "        sorted_right_texts = sorted(right_texts_raw, key=lambda t: (t['x'], t['y']))\n",
        "\n",
        "        # å»é™¤ç©ºç™½ï¼Œä¸¦è½‰ç‚ºlist\n",
        "        left_texts = [t['text'] for t in sorted_left_texts if t['text'].strip() != '']\n",
        "        right_texts = [t['text'] for t in sorted_right_texts if t['text'].strip() != '']\n",
        "\n",
        "        # å°‡æ–‡å­—å„è‡ªä¸²æ¥èµ·ä¾†ï¼Œä¸¦åŠ ä¸Šæ›è¡Œç¬¦è™Ÿ\n",
        "        if left_texts:\n",
        "             left_column_output += ' '.join(left_texts) + '\\n'\n",
        "        if right_texts:\n",
        "             right_column_output += ' '.join(right_texts) + '\\n'\n",
        "\n",
        "    # 2.2 **ä¸åˆ†é‚Š** çš„å‹•æ…‹åƒæ•¸è¨­å®š================================\n",
        "    processed_data_unmerged = [] # ç”¨ä¾†å„²å­˜æ‰€æœ‰å·²ç¶“ç¢ºå®šç‚ºä¸€è¡Œçš„æ•¸æ“šçµæ§‹\n",
        "    column_output = ''\n",
        "    # è¨­ç½®ç‚ºåœ–ç‰‡é«˜åº¦çš„ 1.1%ã€‚å¦‚æœæ–‡å­—å¾ˆå°ï¼Œå¯èƒ½éœ€è¦é™ä½åˆ° 0.01ã€‚åˆ¤æ–·å…©å€‹æ–‡å­—å¡Šæ˜¯å¦å±¬æ–¼åŒä¸€è¡Œçš„ Y è»¸è·é›¢\n",
        "    row_tolerance_unmerged = h * 0.011\n",
        "\n",
        "    for bbox, text, conf in results: # åº§æ¨™é»(å…±æœ‰å››é»å·¦ä¸Š[0]ã€å³ä¸Š[1]ã€å³ä¸‹[2]ã€å·¦ä¸‹[3]é †æ™‚é‡)ã€æ–‡å­—å…§å®¹ã€ä¿¡å¿ƒç¨‹åº¦\n",
        "        # å–å¾—é‚Šç•Œæ¡†çš„ä¸­å¿ƒé» Y å’Œ X åº§æ¨™ï¼Œä¸¦ä¸”æŠŠæ¯å€‹bboxå’Œtextéƒ½æ‹¿å‡ºä¾†åˆ¤æ–·\n",
        "        y_center = (bbox[0][1] + bbox[2][1]) / 2 # åˆ¤æ–·è¾¨è­˜å€å¡Šçš„Yè»¸ä¸­å¿ƒé»\n",
        "        x_center = (bbox[0][0] + bbox[1][0]) / 2 # åˆ¤æ–·è¾¨è­˜å€å¡Šçš„Xè»¸ä¸­å¿ƒé»\n",
        "\n",
        "        column = 'center'\n",
        "\n",
        "        # åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€çµ„\n",
        "        merged = False # åˆä½µé–‹é—œ\n",
        "        for item in processed_data_unmerged:\n",
        "            if abs(y_center - item['y']) < row_tolerance_unmerged: # å¦‚æœYè»¸å’Œè©²è¼ªçš„å€å¡ŠYè»¸ä¸­å¿ƒå°æ–¼åˆ¤æ–·è·é›¢ï¼Œå‰‡åˆ¤å®šç‚ºåŒä¸€å¥è©±ï¼Œå¦‚æœæ˜¯åŒä¸€è¡Œï¼Œå°‡ç•¶å‰çš„æ–‡å­—å¡Šæ·»åŠ åˆ°å·²å­˜åœ¨çš„itemä¸­\n",
        "                item['texts'].append({'text': text, 'x': x_center, 'col': column, 'y' : y_center})\n",
        "                merged = True\n",
        "                break # æ‰¾åˆ°æ‡‰è©²æ”¾åˆ°å“ªä¸€è¡Œå¾Œï¼Œå‰‡è·³å‡ºæ­¤è¼ªå¾ªç’°\n",
        "\n",
        "        if not merged: # å¦‚æœè¿´åœˆçµæŸéƒ½æ‰¾ä¸åˆ°åŒ¹é…çš„è¡Œï¼Œå‰‡å‰µå»ºä¸€å€‹æ–°çš„è¡Œ\n",
        "            processed_data_unmerged.append({\n",
        "                'y': y_center,\n",
        "                'texts': [{'text': text, 'x': x_center, 'col': column, 'y' : y_center}]\n",
        "            }) # ç´€éŒ„å€å¡ŠYè»¸ä¸­å¿ƒé»\n",
        "    # æ’åºèˆ‡æ ¼å¼åŒ–è¼¸å‡º\n",
        "    processed_data_unmerged = sorted(processed_data_unmerged, key = lambda r : r['y']) # å–å‡º yå€¼é€²è¡Œæ’åº\n",
        "\n",
        "    for row in processed_data_unmerged:\n",
        "        texts_raw = [t for t in row['texts']] # æ”¾åˆ°listä¸­\n",
        "        sorted_texts = sorted(texts_raw, key=lambda t: (t['x'], t['y'])) # æ’åºï¼Œxä½ç½®å…ˆï¼Œyä½ç½®å¾Œ\n",
        "\n",
        "        # å°‡æ–‡å­—å„è‡ªä¸²æ¥èµ·ä¾†ï¼Œä¸¦åŠ ä¸Šæ›è¡Œç¬¦è™Ÿ\n",
        "        sorted_texts_list = [t['text'] for t in sorted_texts if t['text'].strip() != '']\n",
        "        if sorted_texts_list:\n",
        "            column_output += ' '.join(sorted_texts_list) + '\\n'\n",
        "\n",
        "    # æœ€çµ‚è¼¸å‡ºçµ¦ AI çš„æ ¼å¼\n",
        "    ocr_text_output = '--- OCR åœ–ç‰‡å…§å®¹æå–çµæœï¼ˆå·¦å³åˆ†æ¬„ï¼‰---\\n'\n",
        "    ocr_text_output += '\\n=== å·¦æ¬„å…§å®¹ (å„ªå…ˆé–±è®€) ===\\n' + left_column_output.strip()\n",
        "    ocr_text_output += '\\n\\n=== å³æ¬„å…§å®¹ (æ¬¡è¦é–±è®€) ===\\n' + right_column_output.strip()\n",
        "    ocr_text_output += '\\n\\n\\n=== ä¸åˆ†æ¬„å…§å®¹ (å¦å¤–ç‰ˆæœ¬) ===\\n' + column_output.strip()\n",
        "    ocr_text_output += '\\n--------------------------------------'\n",
        "\n",
        "    return ocr_text_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-NBUUKANSP3"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFuJwRfjQE9G"
      },
      "outputs": [],
      "source": [
        "cwddate = datetime.strftime(datetime.today(), '%Y%m%d')\n",
        "print(f'é–‹å§‹æŠ“å–ï¼ŒæŠ“å–è³‡æ–™æ—¥æœŸ {cwddate}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTo7duBGQLZB"
      },
      "source": [
        "## ç›¸é—œè·¯å¾‘è¨­ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yatDHhh_mWuj"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# I. å­˜æª”è·¯å¾‘ - googleé›²ç«¯è·¯å¾‘\n",
        "# ====================================================================\n",
        "drive.mount('/content/drive')\n",
        "filepath = pp(r'/content/drive/MyDrive/tibame_proj')\n",
        "\n",
        "# ====================================================================\n",
        "# II. ç›®æ¨™ç¶²å€\n",
        "# ====================================================================\n",
        "urlpath = r'https://www.songshanculturalpark.org/exhibition'\n",
        "jpgparpath = r'https://www.songshanculturalpark.org/gallery/'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Me7K0NTU7N"
      },
      "source": [
        "## å±•è¦½é é¢æ¸…å–®æŠ“å–"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikFciBcuT5It"
      },
      "source": [
        "### ç’°å¢ƒè¨­ç½® - user_agentè®€å–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSnfhr-gT3lo"
      },
      "outputs": [],
      "source": [
        "# USER_AGENT = userdata.get('USER_AGENT') # è¨»è§£æ‰åŸå§‹è®€å– secrets çš„æ–¹å¼\n",
        "\n",
        "# å¾ Colab Secrets è®€å–ä¸¦è¨­å®šç‚ºç’°å¢ƒè®Šæ•¸\n",
        "os.environ['USER_AGENT'] = userdata.get('USER_AGENT')\n",
        "# å¾ç’°å¢ƒè®Šæ•¸ä¸­è®€å– USER_AGENT\n",
        "USER_AGENT = os.environ.get('USER_AGENT')\n",
        "\n",
        "\n",
        "hd = {'user-agent' : USER_AGENT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhbo71-yT8NN"
      },
      "source": [
        "### é€£çµç¶²å€æŠ“å–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "477HuNtDck93"
      },
      "outputs": [],
      "source": [
        "res = req.get(urlpath, headers = hd)\n",
        "soup = bs(res.text, 'html.parser')\n",
        "# å±•è¦½é é¢é€£çµ\n",
        "infourl = [urljoin(urlpath, i.find('a', class_='btn')['href']) for i in soup.find_all('span', class_='row_rt')] # ç­‰ç­‰è¦çˆ¬çš„æ‰€æœ‰ç¶²é é€£çµ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inpIsXpgTujn"
      },
      "source": [
        "## æ¯å€‹å±•è¦½é é¢å…§å®¹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rjxhv2SUMgB"
      },
      "source": [
        "### æ–‡å­—è³‡è¨Š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZxQJoZiTti7"
      },
      "outputs": [],
      "source": [
        "anns = baseinfo(infourl, hd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0aoKZ0mUG7q"
      },
      "source": [
        "### åœ–ç‰‡è³‡è¨Š - OCR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0SzykbfULWw"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    ocr_reader = easyocr.Reader(['ch_tra', 'en']) # å»ºç«‹æ¨¡å‹\n",
        "    print('Info : EasyOCR æ¨¡å‹è¼‰å…¥æˆåŠŸã€‚')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f'Error : EasyOCR æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}')\n",
        "    ocr_reader = None # å¦‚æœå¤±æ•—ï¼Œå°‡å…¶è¨­ç½®ç‚º None\n",
        "\n",
        "for item in anns:\n",
        "    for idx, img in enumerate(item['pageimgurl']):\n",
        "        print(f'=== OCRè¾¨è­˜ä¸­ =========== {idx + 1} / {len(item['pageimgurl'])} ==== {item['name']} {img}') # è®€å–é€²åº¦æ¢ï¼Œé¿å…æˆ‘è¦ºå¾—ä»–ç•¶æ‰äº†\n",
        "        ocrtext = eocr_process(img, ocr_reader) if img else None\n",
        "        item['pagetext'] += ('\\næ¥ä¸‹ä¾†æ˜¯åœ–ç‰‡OCRå…§å®¹æ–‡å­—ï¼Œ' + ocrtext) # è£½ä½œè¦è®€å–çš„æ–‡æœ¬å…§å®¹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ZPTci1VLss"
      },
      "source": [
        "## ç”¨Geminiå¹«æˆ‘èƒå–ä¸€ä¸‹è³‡è¨Š"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK8eQj2zVgJ0"
      },
      "source": [
        "### ç’°å¢ƒè¨­ç½® - gemini api key value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb691a32"
      },
      "source": [
        "# æ‚¨çš„å°ˆæ¡ˆåç¨±\n",
        "\n",
        "## å°ˆæ¡ˆæè¿°\n",
        "ç°¡è¦èªªæ˜æ‚¨çš„å°ˆæ¡ˆæ˜¯åšä»€éº¼çš„ï¼Œä¾‹å¦‚ï¼š\n",
        "é€™å€‹å°ˆæ¡ˆæ˜¯ä¸€å€‹ä½¿ç”¨ Python çˆ¬å–å±•è¦½è³‡è¨Šã€é€²è¡Œ OCR æ–‡å­—è­˜åˆ¥ä¸¦ä½¿ç”¨ Gemini API æå–çµæ§‹åŒ–æ•¸æ“šçš„ç¯„ä¾‹å°ˆæ¡ˆã€‚\n",
        "\n",
        "## ç’°å¢ƒè¨­å®š\n",
        "\n",
        "æœ¬å°ˆæ¡ˆéœ€è¦ä½¿ç”¨åˆ° Google Colab ç’°å¢ƒï¼Œä¸¦ä¾è³´ä»¥ä¸‹æ•æ„Ÿè³‡è¨Šï¼Œé€™äº›è³‡è¨Šéœ€è¦å„²å­˜åœ¨ Colab çš„ Secrets ä¸­ï¼š\n",
        "\n",
        "1.  **`USER_AGENT`**: ç”¨æ–¼ HTTP è«‹æ±‚çš„ User-Agent å­—ä¸²ã€‚\n",
        "2.  **`GEMINI_API_KEY`**: æ‚¨çš„ Google Gemini API é‡‘é‘°ï¼Œç”¨æ–¼æ–‡å­—åˆ†æã€‚\n",
        "\n",
        "è«‹ä¾ç…§ä»¥ä¸‹æ­¥é©Ÿåœ¨æ‚¨çš„ Colab ç’°å¢ƒä¸­è¨­å®š Secretsï¼š\n",
        "\n",
        "1.  é–‹å•Ÿæ‚¨çš„ Colab ç­†è¨˜æœ¬ã€‚\n",
        "2.  åœ¨å·¦å´é¢æ¿ä¸­ï¼Œé»æ“Šã€ŒğŸ”‘ å¯†é‘°ã€åœ–æ¨™ã€‚\n",
        "3.  é»æ“Šã€Œæ–°å¢å¯†é‘°ã€ã€‚\n",
        "4.  åœ¨ã€Œåç¨±ã€æ¬„ä½è¼¸å…¥ `USER_AGENT`ï¼Œåœ¨ã€Œå€¼ã€æ¬„ä½è¼¸å…¥æ‚¨çš„ User-Agent å­—ä¸²ã€‚é»æ“Šã€Œå„²å­˜ã€ã€‚\n",
        "5.  å†æ¬¡é»æ“Šã€Œæ–°å¢å¯†é‘°ã€ã€‚\n",
        "6.  åœ¨ã€Œåç¨±ã€æ¬„ä½è¼¸å…¥ `GEMINI_API_KEY`ï¼Œåœ¨ã€Œå€¼ã€æ¬„ä½è¼¸å…¥æ‚¨çš„ Gemini API é‡‘é‘°ã€‚é»æ“Šã€Œå„²å­˜ã€ã€‚\n",
        "7.  ç¢ºä¿åœ¨ç­†è¨˜æœ¬ä¸­åŸ·è¡Œäº†è®€å– Secrets ä¸¦è¨­å®šç’°å¢ƒè®Šæ•¸çš„ç¨‹å¼ç¢¼å–®å…ƒã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs_0wlHiVRC1"
      },
      "outputs": [],
      "source": [
        "# GEMINI_KEY = userdata.get('GEMINI_API_KEY') # è¨»è§£æ‰åŸå§‹è®€å– secrets çš„æ–¹å¼\n",
        "\n",
        "# å¾ Colab Secrets è®€å–ä¸¦è¨­å®šç‚ºç’°å¢ƒè®Šæ•¸\n",
        "os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')\n",
        "# å¾ç’°å¢ƒè®Šæ•¸ä¸­è®€å– GEMINI_API_KEY\n",
        "GEMINI_KEY = os.environ.get('GEMINI_API_KEY')\n",
        "\n",
        "client = genai.Client(api_key = GEMINI_KEY)\n",
        "\n",
        "exteninfo = [] # ç”¨ä¾†å„²å­˜æå–åˆ°çš„çµæ§‹åŒ–æ•¸æ“š\n",
        "cantcatch = [] # æ²’æœ‰æŠ“åˆ°çš„è³‡æ–™è¨˜éŒ„ç”¨\n",
        "MAX_RETRIES = 5  # è¨­å®šæœ€å¤§é‡è©¦æ¬¡æ•¸\n",
        "INITIAL_DELAY = 10 # å‰›é–‹å§‹çš„ç­‰å¾…ç§’æ•¸\n",
        "\n",
        "class EmptyResponseError(Exception):\n",
        "    '''è‡ªå®šç¾©éŒ¯èª¤ï¼šç•¶ API å›å‚³ç©ºçš„æ–‡å­—å…§å®¹æ™‚æ‹‹å‡ºã€‚'''\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABf-6igIVvXm"
      },
      "source": [
        "### çµ±ä¸€Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esFPGQGMV3cJ"
      },
      "outputs": [],
      "source": [
        "# å®šç¾© çµæ§‹åŒ–ç”¢å‡º (Structured Output): å¼·åˆ¶ Geminiåœ¨è™•ç†ä¸€æ®µæ–‡å­—è³‡è¨Šå¾Œï¼Œå¿…é ˆä»¥ä¸€å€‹å›ºå®šä¸”å¯é æ¸¬çš„ JSON æ ¼å¼å›å‚³çµæœï¼Œè€Œä¸æ˜¯è‡ªç”±å½¢å¼çš„æ–‡å­—ã€‚\n",
        "extraction_schema = types.Schema(\n",
        "    type = types.Type.OBJECT, # ä½¿ç”¨ OBJECT ä½œç‚ºå–®ä¸€æ´»å‹•çš„å®¹å™¨\n",
        "    properties = {\n",
        "        'name': types.Schema(type = types.Type.STRING, description = 'æ´»å‹•çš„ä¸»è¦åç¨±æˆ–æ¨™é¡Œã€‚'),\n",
        "\n",
        "        'wrktime': types.Schema(type = types.Type.STRING, description = 'æ´»å‹•çš„é–‹æ”¾æˆ–ç‡Ÿæ¥­æ™‚é–“ï¼Œéœ€åŒ…å«ä¸åŒæ—¥æœŸçš„è®ŠåŒ–å’Œæœ€å¾Œå…¥å ´æ™‚é–“çš„èªªæ˜ï¼Œè«‹çµ±ä¸€ä½¿ç”¨ 24 å°æ™‚åˆ¶ï¼Œä¾‹å¦‚ **10:00 - 17:00**ã€‚'),\n",
        "\n",
        "        'price': types.Schema(type = types.Type.STRING, description = 'ç¥¨å‹™æˆ–å…¥å ´è³‡è¨Šï¼Œå¦‚æœå…è²»è«‹å¯« **å…è²»å…¥å ´**ã€‚'),\n",
        "\n",
        "        'note': types.Schema(type = types.Type.STRING, description = 'å¦‚æœå±•è¦½æ˜¯å¤šå€‹é …ç›®çµ„æˆï¼Œå‰‡å°‡å„è‡ªçš„è³‡è¨Šå­˜æ”¾æ–¼æ­¤ï¼Œéœ€è¦çš„å…§å®¹ç‚º**åç¨±(name)**ã€**æ—¥æœŸ(date)**ã€**æ™‚é–“(wrktime)**ã€**ç¥¨åƒ¹(price)**'),\n",
        "\n",
        "        'url': types.Schema(\n",
        "            type = types.Type.ARRAY,\n",
        "            description = 'æ´»å‹•ç›¸é—œçš„æ‰€æœ‰é‡è¦ç¶²å€(å®˜ç¶²ã€FBã€è³¼ç¥¨é€£çµç­‰)ã€‚',\n",
        "            items = types.Schema(type = types.Type.STRING, description = 'å®Œæ•´çš„ URLã€‚') # URL å­—ä¸²çš„é™£åˆ—\n",
        "        )\n",
        "    },\n",
        "    required = ['name', 'wrktime', 'price'] # å¿…è¦æ¬„ä½\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeqhNJ6jWPCk"
      },
      "source": [
        "### Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lOMOC3pWTYj"
      },
      "outputs": [],
      "source": [
        "base_prompt = '''\n",
        "æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„æ•¸æ“šåˆ†æå¸«ï¼Œä»¥åŠå±•è¦½ç­–å±•äººå“¡ã€‚æ‚¨çš„ä»»å‹™æ˜¯å¾æä¾›çš„å–®ä¸€æ–‡æœ¬å…§å®¹ä¸­ï¼Œè­˜åˆ¥ä¸¦åš´æ ¼æå–æ‰€æœ‰æ´»å‹•è³‡è¨Š(åªè¦å±•æ¼”æ´»å‹•ï¼Œä¸è¦å„ªæƒ è³‡è¨Š)ã€‚\n",
        "1.è«‹å°‡æå–çš„çµæœå°è£ç‚ºå–®ä¸€å€‹ JSON ç‰©ä»¶ï¼Œä¸¦éµå¾ªæˆ‘æŒ‡å®šçš„ JSON Schema æ ¼å¼ã€‚\n",
        "2.è«‹åªè¿”å› JSON æ ¼å¼çš„å…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•å¤šé¤˜çš„è§£é‡‹æˆ–æ–‡å­—ã€‚å¦‚æœåªæ‰¾åˆ°éƒ¨åˆ†æ¬„ä½è³‡è¨Šï¼Œå°±å¡«å…¥é€™å¹¾å€‹æ¬„ä½å°±å¥½ï¼Œå…¶ä»–å¡«å…¥**ç„¡è³‡è¨Š**ã€‚\n",
        "3.å…§å®¹å› ç‚ºæ˜¯è©²å±•è¦½è² è²¬äººå“¡å¡«å¯«ï¼Œæ‰€ä»¥å¯«æ³•éƒ½ä¸åŒï¼Œå¦‚æœèªç‚ºæ–‡æœ¬ä¸­æ²’æœ‰ç›¸é—œæ¬„ä½è³‡è¨Šæˆ–å…§å®¹æ˜¯ç©ºç™½ï¼Œå‰‡åœ¨æ¬„ä½ä¸­å¡«å…¥**ç„¡è³‡è¨Šã€‚\n",
        "4.æˆ‘æœƒä¸€ä½µçµ¦ä½ è©²å±•è¦½æå ±çš„ç°¡å–®è³‡è¨Šï¼Œä½ å¯ä»¥ç•¶ä½œåƒè€ƒé—œéµå­—ï¼Œæ”¾åœ¨æœ€å¾Œä¸¦ç”¨**{}**åŒ…èµ·ä¾†ã€‚\n",
        "5.å¦‚æœé‡åˆ°åç¨±ä¸­æœ‰ **å±•è¦½æ”»ç•¥** é é¢ï¼Œå‰‡è®€å–æ–‡æœ¬ä¸­æœ€ä¸Šé¢é–‹é ­æ˜¯ **å±•æ¼”æ´»å‹•** é …ä¸‹çš„è³‡è¨Šï¼Œçœ‹åˆ°é–‹é ­æ˜¯**å¿«é–ƒé™å®šã€è³£åº—ä¸»é¡Œã€å·¥ä½œåŠ**ç­‰é—œéµå­—ï¼Œä»¥ä¸‹å…§å®¹éƒ½æ’é™¤ã€‚\n",
        "6.é‡åˆ°å±•æ¼”æ´»å‹•å…§éƒ¨æœ‰å¾ˆå¤šæ´»å‹•ï¼Œè«‹æ•´ç†å¾Œç”¨åˆ—é»æ–¹å¼æ”¾å…¥**å‚™è¨»**æ¬„ä½ä¸­ï¼Œåˆ—é»å…§å®¹åŒ…æ‹¬**åç¨±**ã€**é–‹æ”¾æ—¥æœŸ**ã€**é–‹æ”¾æ™‚é–“**ã€**ç¥¨åƒ¹**ï¼Œä¸è¦æœ‰ä»»ä½•å¤šé¤˜çš„è§£é‡‹æˆ–æ–‡å­—\n",
        "\n",
        "\n",
        "ä»¥ä¸‹æ˜¯å¾…åˆ†æçš„æ´»å‹•æ–‡æœ¬ï¼Œä»¥åŠæœ€å¾Œç”¨{}æ¨™ä½èµ·ä¾†çš„è©²äººå“¡æä¾›çš„ç°¡å–®è³‡è¨Šï¼š\n",
        "'''\n",
        "\n",
        "# è«‹æ¨¡å‹å†æ€è€ƒç”¨\n",
        "CORRECTION_PROMPT = '''\n",
        "\\n[!!!] è­¦å‘Šï¼šæ‚¨ä¸Šä¸€æ¬¡çš„è¼¸å‡ºç„¡æ³•è¢«è§£æç‚ºæœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚\n",
        "è«‹æ‚¨**åš´æ ¼**é‡æ–°æª¢æŸ¥æ‚¨çš„è¼¸å‡ºå…§å®¹ï¼Œä¸¦ç¢ºä¿å®ƒæ˜¯ä¸€å€‹**ç´”æ·¨ã€å®Œæ•´ä¸”ç¬¦åˆ JSON è¦ç¯„**çš„ JSON å­—ä¸²\n",
        "(æœ‰æ²’æœ‰å¯èƒ½æ˜¯å°‘äº†ä¸Šä¸‹ä¸­æ‹¬å¼§æˆ–æ˜¯é€—é»è€Œå·²ï¼Œè«‹æ‚¨æ³¨æ„é€™é»)ï¼Œè«‹ä¸è¦åŒ…å«ä»»ä½•é¡å¤–çš„è§£é‡‹æ€§æ–‡å­—æˆ–å¼•è¨€ã€‚è¬è¬ï¼'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX2_vDQCWnCq"
      },
      "source": [
        "### æ–‡æœ¬åˆ†æ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCb3rC36WsD4"
      },
      "outputs": [],
      "source": [
        "for item in anns:\n",
        "    current_delay = INITIAL_DELAY\n",
        "    text_content = item['pagetext'] # é é¢ä¸Šçš„å…§å®¹æ•˜è¿°\n",
        "    curt_name = item['name'] # é é¢ä¸Šå¯«çš„å±•è¦½åç¨±\n",
        "\n",
        "    # æç¤ºè©å¤–ï¼Œé™„ä¸Šåˆ—è¡¨ä¸Šçš„åŸºç¤è³‡è¨Šç•¶ä½œåˆ¤æ–·ä¾æ“š å…§éƒ¨RAGå·¥ç¨‹\n",
        "    full_prompt = (base_prompt +\n",
        "                   text_content +\n",
        "                   'ï¼Œé€™å€‹æ˜¯å±•è¦½è³‡è¨Š: {åç¨±:' + f'{item['name']}ã€é–‹å§‹æ™‚é–“:{item['strt_dt']}ã€çµæŸæ™‚é–“:{item['end_dt']}ã€å±•è¦½èªªæ˜:{item['article']}ã€åœ°é»{item['loc']}' + '}')\n",
        "\n",
        "    for attempt in range(MAX_RETRIES): # é–‹å§‹æŠ“å–è³‡æ–™ï¼Œæ¯ç­†è³‡æ–™æœ€å¤§æ¬¡æ•¸å°±æ˜¯5æ¬¡ï¼Œè¶…éå°±é€å›éŒ¯èª¤\n",
        "        try:\n",
        "            print(f'Info : é–‹å§‹å˜—è©¦æå–ã€Œ{curt_name}ã€æ´»å‹•è³‡è¨Š (ç¬¬ {attempt + 1}/{MAX_RETRIES} æ¬¡)')\n",
        "\n",
        "            response = client.models.generate_content(\n",
        "                model = 'gemini-2.5-flash-lite', # ä½¿ç”¨æ¨¡å‹ é‡å°é€Ÿåº¦å’Œæ•ˆç‡é€²è¡Œäº†å„ªåŒ–ï¼Œé©ç”¨æ–¼è¼ƒè¼•é‡ç´šæˆ–å°å»¶é²è¦æ±‚é«˜çš„ä»»å‹™\n",
        "                contents = full_prompt, # æç¤ºè©\n",
        "                config=types.GenerateContentConfig( # è¨­å®šæ¨¡å‹å¦‚ä½•å›æ‡‰ï¼ŒåŒ…æ‹¬è¼¸å‡ºæ ¼å¼ã€é™åˆ¶å’Œå‰µé€ æ€§ç¨‹åº¦ç­‰\n",
        "                    response_mime_type = 'application/json', # è¿”å›jsonæ ¼å¼è³‡æ–™\n",
        "                    response_schema = extraction_schema, # å›æ‡‰çš„æ ¼å¼æŒ‰ç…§å‰é¢å®šç¾©çš„è¼¸å‡º\n",
        "                    max_output_tokens = 1024, # é™åˆ¶å›å‚³çš„tokenæ•¸é‡ï¼Œç´„3-4å€‹è‹±æ–‡å­—æ¯æˆ–åŠå€‹ä¸­æ–‡å­—ç­‰æ–¼1å€‹token\n",
        "                    temperature = 0.2 # æ„ˆä½çš„å€¼ä»£è¡¨æ¨¡å‹çš„å›ç­”æ›´å…·æ±ºå®šæ€§ã€æº–ç¢ºå’Œå¯é æ¸¬ï¼Œé©åˆéœ€è¦åš´æ ¼æ•¸æ“šæå–å’Œéµå¾ªæ ¼å¼çš„ä»»å‹™ã€‚è¼ƒé«˜çš„å€¼å‰‡é©ç”¨æ–¼å¯«ä½œã€å‰µæ„æˆ–é ­è…¦é¢¨æš´ã€‚\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # å¢åŠ ä¸€é …æª¢æŸ¥ï¼šç¢ºä¿ response.text æ˜¯å€‹å­—ä¸²\n",
        "            if response is None or response.text is None:\n",
        "                raise EmptyResponseError(f'Error : API è¿”å›äº†ç©ºçš„æ–‡å­—å…§å®¹ã€‚')\n",
        "\n",
        "            # å¦‚æœæˆåŠŸï¼Œè·³å‡ºé‡è©¦å¾ªç’° ==================================================== åˆ°é€™æ­¥ä»£è¡¨æœ‰æŠ“åˆ°è³‡æ–™\n",
        "            extracted_json = json.loads(response.text) # dtype dict\n",
        "            item['extrninfo'] = extracted_json # å°‡ç‡Ÿæ¥­æ™‚é–“ã€ç¥¨åƒ¹ã€å®˜ç¶²ç­‰è³‡è¨Šè£œå……é€²å»\n",
        "            print(f'Successed : ã€Œ{curt_name}ã€æˆåŠŸæå–ï¼š{extracted_json.get('name', 'ç„¡åç¨±')}')\n",
        "            time.sleep(rd.randint(5, 15))\n",
        "            break\n",
        "            # =========================================================================\n",
        "\n",
        "        except (json.JSONDecodeError, EmptyResponseError) as e: # ä¾‹å¤–è™•ç† - å›å‚³å…§å®¹ç‚ºç©ºå€¼\n",
        "            if attempt < MAX_RETRIES - 1:\n",
        "                print(f'Waring : è­¦å‘Šï¼š=== ã€Œ{curt_name}ã€ === æ¨¡å‹æœªè¿”å›æœ‰æ•ˆ JSON (éŒ¯èª¤ï¼š{e})ã€‚é€™æ¬¡å–å¾—çš„å…§å®¹æ˜¯é€™äº›  {response.text}')\n",
        "                print(f'Action : è¦æ±‚æ¨¡å‹è‡ªæˆ‘ä¿®æ­£... ç­‰å¾… 5 ç§’å¾Œé‡è©¦ã€‚')\n",
        "                full_prompt += CORRECTION_PROMPT # å°‡ä¿®æ­£æŒ‡ä»¤é™„åŠ åˆ°æç¤ºè©ä¸­ï¼Œè«‹æ¨¡å‹é‡æ–°æ€è€ƒå¯èƒ½éŒ¯èª¤çš„åœ°æ–¹\n",
        "                time.sleep(5)\n",
        "                continue # ç¹¼çºŒä¸‹ä¸€æ¬¡é‡è©¦ (å¸¶è‘—ä¿®æ­£æç¤º)\n",
        "            else:\n",
        "                print(f'Fail : JSON æ ¼å¼éŒ¯èª¤å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œè·³éæ­¤é …ç›®ã€‚')\n",
        "                cantcatch.append(curt_name) # å°‡æ²’æœ‰æŠ“åˆ°çš„è¨˜éŒ„èµ·ä¾†\n",
        "                time.sleep(rd.randint(5, 15))\n",
        "                break # æŠ“ä¸åˆ°å•¦ï¼Œç›¡åŠ›äº†ï¼Œèµ°å§...\n",
        "        except APIError as e: # ä¾‹å¤–è™•ç† - è¿½è¹¤APIå•é¡Œ\n",
        "            # è™•ç† 503 ç­‰ API éŒ¯èª¤\n",
        "            if attempt < MAX_RETRIES - 1 and 'UNAVAILABLE' in str(e):\n",
        "                print(f'Error : ä¼ºæœå™¨éè¼‰ (503 éŒ¯èª¤)ã€‚ç­‰å¾… {current_delay} ç§’å¾Œé‡è©¦...')\n",
        "                time.sleep(current_delay)\n",
        "                current_delay *= 1.5\n",
        "                continue\n",
        "            else:\n",
        "                print(f'Fail : API å‘¼å«å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæˆ–ç™¼ç”Ÿä¸å¯æ¢å¾©éŒ¯èª¤: {e}')\n",
        "                cantcatch.append(curt_name)\n",
        "                time.sleep(rd.randint(5, 15))\n",
        "                break # æŠ“ä¸åˆ°å•¦ï¼Œç›¡åŠ›äº†ï¼Œèµ°å§...\n",
        "        except Exception as e: # å…¶ä»–æœªçŸ¥éŒ¯èª¤è™•ç†\n",
        "            print(f'Error : ã€Œ{curt_name}ã€ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}')\n",
        "            cantcatch.append(curt_name) # å°‡æ²’æœ‰æŠ“åˆ°çš„è¨˜éŒ„èµ·ä¾†\n",
        "            time.sleep(rd.randint(5, 15))\n",
        "            break\n",
        "    print('***************************')\n",
        "print(f'é€™äº›æ˜¯æ²’æœ‰æŠ“åˆ°çš„å±•è¦½: {cantcatch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rcVz714W20K"
      },
      "source": [
        "# Save Infomation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NVAAWwzdPu5"
      },
      "outputs": [],
      "source": [
        "rmlist = ['big_imgurl', 'pageimgurl', 'pagetext', 'extrninfo']\n",
        "\n",
        "test = copy.deepcopy(anns)\n",
        "savedata = [] # æ–°å¢ä¸€å€‹ list ä¾†å„²å­˜è™•ç†å¾Œçš„è³‡æ–™\n",
        "\n",
        "for ann in test:\n",
        "    for k in ['wrktime', 'price', 'note']:\n",
        "        ann[k] = ann['extrninfo'].get(k, 'ç„¡è³‡è¨Š')\n",
        "    for j in rmlist:\n",
        "        if j in ann: # æª¢æŸ¥éµæ˜¯å¦å­˜åœ¨ï¼Œé¿å… KeyError\n",
        "            del ann[j]\n",
        "    savedata.append(ann) # å°‡è™•ç†å¾Œçš„å­—å…¸æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
        "else:\n",
        "    print(f'Warning: è·³é {ann.get(\"name\", \"Unnamed item\")}ï¼Œå…§å®¹æ²’æœ‰extrninfoæ¬„ä½')\n",
        "\n",
        "output_filename = f'exhibition_info_{datetime.strftime(datetime.today(), \"%Y_%m_%d\")}.json'\n",
        "output_path = filepath / output_filename\n",
        "\n",
        "with open(output_path, encoding = 'utf-8', mode = 'w') as f:\n",
        "        json.dump(savedata, f, indent = 4, ensure_ascii = False)\n",
        "\n",
        "# åŸå§‹çš„åˆ—å°è¿´åœˆå¯ä»¥é¸æ“‡ä¿ç•™æˆ–ç§»é™¤ï¼Œé€™è£¡ä¿ç•™ä¾›åƒè€ƒ\n",
        "# for item in processed_data:\n",
        "#     for key, value in item.items():\n",
        "#         print(f'{key} : {value}')\n",
        "#     print('****************************')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AgaMcyPUN3l4"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}